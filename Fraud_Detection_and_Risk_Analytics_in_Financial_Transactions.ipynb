{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "watP0bYUBH8H"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Fraud Detection and Risk Analytics in Financial Transactions\n",
        "Tech: Python, Sklearn, XGBoost, Matplotlib\n",
        "\n",
        "This pipeline provides:\n",
        " - Data loading (CSV with transaction features + fraud label)\n",
        " - Preprocessing (scaling, class balancing via SMOTE if needed)\n",
        " - Model training with RandomForest and XGBoost\n",
        " - Evaluation with precision, recall, F1, ROC-AUC\n",
        " - Feature importance visualization\n",
        " - Fraud risk trend visualization (matplotlib)\n",
        "\n",
        "Usage:\n",
        " python fraud_detection_risk_analytics.py --data_file transactions.csv --label_col fraud\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMB_AVAILABLE = False\n",
        "\n",
        "\n",
        "def load_and_prepare_data(file, label_col, test_size=0.2, scale=True, balance=True):\n",
        "    df = pd.read_csv(file)\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=42)\n",
        "\n",
        "    if balance and IMB_AVAILABLE:\n",
        "        sm = SMOTE(random_state=42)\n",
        "        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "    else:\n",
        "        X_train, X_test = X_train.values, X_test.values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, list(X.columns)\n",
        "\n",
        "\n",
        "def train_models(X_train, y_train):\n",
        "    models = {}\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=200, max_depth=None, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    models[\"RandomForest\"] = rf\n",
        "\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        scale_pos_weight=1,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    models[\"XGBoost\"] = xgb_model\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    preds = model.predict(X_test)\n",
        "    probs = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else preds\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average=\"binary\")\n",
        "    auc = roc_auc_score(y_test, probs)\n",
        "\n",
        "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
        "    print(classification_report(y_test, preds, digits=4))\n",
        "    print(\"ROC-AUC:\", round(auc, 4))\n",
        "\n",
        "    # Confusion matrix plot\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{model_name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"roc_auc\": auc}\n",
        "\n",
        "\n",
        "def plot_feature_importances(model, feature_names, model_name, top_n=15):\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1][:top_n]\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(range(top_n), importances[indices], align=\"center\")\n",
        "        plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha=\"right\")\n",
        "        plt.title(f\"{model_name} Top {top_n} Feature Importances\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_fraud_trends(df, date_col=\"date\", label_col=\"fraud\"):\n",
        "    if date_col not in df.columns:\n",
        "        print(\"Date column not found in dataset; skipping trend plot.\")\n",
        "        return\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "    trend = df.groupby(df[date_col].dt.to_period(\"M\"))[label_col].mean()\n",
        "    trend.plot(kind=\"line\", marker=\"o\")\n",
        "    plt.ylabel(\"Fraud Rate\")\n",
        "    plt.title(\"Fraud Trends Over Time\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    X_train, X_test, y_train, y_test, feature_names = load_and_prepare_data(args.data_file, args.label_col)\n",
        "\n",
        "    models = train_models(X_train, y_train)\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        results[name] = evaluate_model(model, X_test, y_test, name)\n",
        "        plot_feature_importances(model, feature_names, name)\n",
        "\n",
        "    # Fraud trend visualization if date col exists\n",
        "    df = pd.read_csv(args.data_file)\n",
        "    if \"date\" in df.columns:\n",
        "        plot_fraud_trends(df, date_col=\"date\", label_col=args.label_col)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Fraud Detection and Risk Analytics\")\n",
        "    parser.add_argument(\"--data_file\", type=str, required=True, help=\"Path to transactions CSV\")\n",
        "    parser.add_argument(\"--label_col\", type=str, required=True, help=\"Column name for fraud label\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ]
    }
  ]
}